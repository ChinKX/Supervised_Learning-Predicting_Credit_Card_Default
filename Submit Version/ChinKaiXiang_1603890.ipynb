{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Supervised Learning\n",
    "## Dataset: Default of Credit Card Clients\n",
    "Dataset Link: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
    "\n",
    "### <font color=red>Note that:<br>- The hyperparameter tuning cells are changed to markdown, please change the cell type to run the code<br>- The results of hyperparameter tuning might not be the same as well</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('default_of_credit_card_clients.xls', index_col=\"ID\", skiprows=[0])\n",
    "\n",
    "#print(df.head())\n",
    "#print(df.describe())\n",
    "#print(df.info())\n",
    "\n",
    "# One Hot-Coding for categorical features : binary features take values of 1 or 0\n",
    "# - Scikit-learn might assume these are numerical features\n",
    "# - can't use labels because Scikit-learn only accepts numbers\n",
    "\n",
    "# obtain the one hot encoding of columns 'SEX', 'EDUCATION', 'MARRIAGE'\n",
    "# The base values are: female, other_education, other_marital_status\n",
    "df['male'] = (df['SEX'] == 1).astype('int')\n",
    "df.drop('SEX', axis=1, inplace=True)\n",
    "\n",
    "df['grad_school'] = (df['EDUCATION'] == 1).astype('int')\n",
    "df['university'] = (df['EDUCATION'] == 2).astype('int')\n",
    "df['high_school'] = (df['EDUCATION'] == 3).astype('int')\n",
    "df.drop('EDUCATION', axis=1, inplace=True)\n",
    "\n",
    "df['married'] = (df['MARRIAGE'] == 1).astype('int')\n",
    "df['single'] = (df['MARRIAGE'] == 2).astype('int')\n",
    "df.drop('MARRIAGE', axis=1, inplace=True)\n",
    "\n",
    "# From the documentation, we can infer that PAY_n features represent not delayed if it is <= 0\n",
    "pay_n_features = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "for col in pay_n_features:\n",
    "    hist = df[col].hist(bins=10)\n",
    "    print(\"Plotting for column {}\".format(col))\n",
    "    plt.show()\n",
    "# modify all values of PAY_n features which are < 0 to 0\n",
    "for pay_n in pay_n_features:\n",
    "    df.loc[df[pay_n] <= 0, pay_n] = 0\n",
    "\n",
    "df.rename(columns={'default payment next month': 'default'}, inplace=True)\n",
    "    \n",
    "pd.options.display.max_columns = None\n",
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature scaling to get more accurate representation and better learning performance\n",
    "'''\n",
    "Most machine learning algorithms take into account only the magnitude of the measurements, not the units of those measurements.\n",
    "The feature with a very high magnitude (number) may affect the prediction a lot more than an equally important feature.\n",
    "e.g. the AGE (within certain fixed range) and the PAY_AMTn (monetary) features have very different ranges of values\n",
    "\n",
    "RobustScaler:\n",
    "The Robust Scaler uses statistics that are robust to outliers.\n",
    "This usage of interquartiles means that they focus on the parts where the bulk of the data is.\n",
    "This makes them very suitable for working with outliers.\n",
    "Notice that after Robust scaling, the distributions are brought into the same scale and overlap, but the outliers remain outside of bulk of the new distributions.\n",
    "'''\n",
    "# plot the distribution of all data\n",
    "for col in df.columns:\n",
    "    hist = df[col].hist(bins=10)\n",
    "    print(\"Plotting for column {}\".format(col))\n",
    "    plt.show()\n",
    "\n",
    "x = df.drop('default', axis=1)\n",
    "rb_scaler = RobustScaler()\n",
    "x = rb_scaler.fit_transform(x)# rescale all the features to a same range\n",
    "y = df['default']\n",
    "# stratify parameter makes data split in a stratified fashion meaning the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=123, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_matrix(CM, labels=['pay', 'default']):\n",
    "    df = pd.DataFrame(data = CM, index=labels, columns=labels)\n",
    "    df.index.name = 'TRUE'\n",
    "    df.columns.name = 'PREDICTION'\n",
    "    df.loc['Total'] = df.sum()\n",
    "    df['Total'] = df.sum(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing dataframe to store the evaluation metrics\n",
    "metrics = pd.DataFrame(\n",
    "    index=['accuracy', 'precision', 'recall', 'f1-score', 'AUC'],\n",
    "    columns=['LogisticReg', 'DecisionTree', 'NeuralNet']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>1. Logistic Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Logistic Regression (Using GridSearchCV)\n",
    "Tuning regularization penalty and regularization hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create an instance of the model\n",
    "log_reg = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "#logspace => Return numbers spaced evenly on a log scale\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# Create hyperparameter options\n",
    "params = dict(C=C, penalty=penalty)\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf = GridSearchCV(log_reg, params, verbose=0, cv=3)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(x_train, y_train)\n",
    "\n",
    "# Display the best score and best parameters\n",
    "print(\"Best mean test score and best parameters:\")\n",
    "print(clf.best_score_, clf.best_params_)\n",
    "print()\n",
    "\n",
    "# Loop through and display each pair of mean test score and parameter\n",
    "print(\"List of Mean test scores and respective parameters:\")\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "parameters = clf.cv_results_['params']\n",
    "for mean, parameter in zip(means, parameters):\n",
    "    print(mean, parameter)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import the model class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create an instance of the model\n",
    "log_reg = LogisticRegression(n_jobs=-1, C=2.7825594022071245, penalty='l1')\n",
    "\n",
    "# train the model using the training data\n",
    "log_reg.fit(x_train, y_train)\n",
    "\n",
    "## evaluate the model performance and log the metrics\n",
    "y_predicted = log_reg.predict(x_test)\n",
    "probs_log_reg = log_reg.predict_proba(x_test)# predict probabilities\n",
    "probs_log_reg = probs_log_reg[:, 1]# keep probabilities for the positive outcome only\n",
    "metrics.loc['accuracy', 'LogisticReg'] = accuracy_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['precision', 'LogisticReg'] = precision_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['recall', 'LogisticReg'] = recall_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['f1-score', 'LogisticReg'] = f1_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['AUC', 'LogisticReg'] = roc_auc_score(y_test, probs_log_reg)\n",
    "\n",
    "# construct the confusion matrix\n",
    "CM = confusion_matrix(y_pred=y_predicted, y_true=y_test)\n",
    "c_matrix(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>2. Decision Tree Classifier</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Decision Tree Classifier (Using GridSearchCV)\n",
    "- Tuning max depth, min sample split and criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# create an instance of the model\n",
    "dec_tree = DecisionTreeClassifier(min_samples_split=30, min_samples_leaf=10)\n",
    "\n",
    "# Create max depth space\n",
    "depths = [None, 5, 10]\n",
    "\n",
    "# Create minimum sample split space\n",
    "samples_splits = [2, 4]\n",
    "\n",
    "# Create criterion space\n",
    "criteria = ['gini', 'entropy']\n",
    "\n",
    "# Create hyperparameter options\n",
    "params = dict(max_depth=depths, min_samples_split=samples_splits, criterion=criteria)\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf = GridSearchCV(dec_tree, params, verbose=0, cv=3)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(x_train, y_train)\n",
    "\n",
    "# Display the best score and best parameters\n",
    "print(\"Best mean test score and best parameters:\")\n",
    "print(clf.best_score_, clf.best_params_)\n",
    "print()\n",
    "\n",
    "# Loop through and display each pair of mean test score and parameter\n",
    "print(\"List of Mean test scores and respective parameters:\")\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "parameters = clf.cv_results_['params']\n",
    "for mean, parameter in zip(means, parameters):\n",
    "    print(mean, parameter)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model class\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# create an instance of the model\n",
    "'''\n",
    "min_samples_split => minimum number of samples required to split an internal node\n",
    "min_samples_leaf => minimum number of samples required to be at a leaf node\n",
    "'''\n",
    "dec_tree = DecisionTreeClassifier(criterion='gini', max_depth=5, min_samples_split=2, min_samples_leaf=10)\n",
    "\n",
    "# train the model using the training data\n",
    "dec_tree.fit(x_train, y_train)\n",
    "\n",
    "## evaluate the model performance and log the metrics\n",
    "y_predicted = dec_tree.predict(x_test)\n",
    "probs_dec_tree = dec_tree.predict_proba(x_test)# predict probabilities\n",
    "probs_dec_tree = probs_dec_tree[:, 1]# keep probabilities for the positive outcome only\n",
    "metrics.loc['accuracy', 'DecisionTree'] = accuracy_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['precision', 'DecisionTree'] = precision_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['recall', 'DecisionTree'] = recall_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['f1-score', 'DecisionTree'] = f1_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['AUC', 'DecisionTree'] = roc_auc_score(y_test, probs_dec_tree)\n",
    "\n",
    "# construct the confusion matrix\n",
    "CM = confusion_matrix(y_pred=y_predicted, y_true=y_test)\n",
    "c_matrix(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>3. Feed Forward Deep Neural Networks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Sequential Model (Using GridSearchCV)\n",
    "To use Keras model in Scikit Learn, we need to use the KerasClassifier or KerasRegressor classes. These two classes accept a function which creates and returns a Keras model.\n",
    "1. Tuning batch size and epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.constraints import unit_norm\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def cc_default_classifier():\n",
    "    input_dim = x_train.shape[1]\n",
    "\n",
    "    '''\n",
    "    Weight constraints (unit_norm) provide an approach to reduce the overfitting of a deep learning neural network model on the training data and improve the performance of the model on new data\n",
    "    '''\n",
    "    neuralNet = Sequential()\n",
    "    neuralNet.add(Dense(64, input_shape=(input_dim,), activation='relu', kernel_constraint=unit_norm()))\n",
    "    neuralNet.add(Dropout(0.5))\n",
    "    neuralNet.add(Dense(32, activation='relu', kernel_constraint=unit_norm()))\n",
    "    neuralNet.add(Dropout(0.5))\n",
    "    neuralNet.add(Dense(32, activation='relu', kernel_constraint=unit_norm()))\n",
    "    neuralNet.add(Dropout(0.5))\n",
    "    neuralNet.add(Dense(16, activation='relu', kernel_constraint=unit_norm()))\n",
    "    neuralNet.add(Dropout(0.5))\n",
    "    neuralNet.add(Dense(1,  activation='sigmoid'))\n",
    "\n",
    "    neuralNet.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return neuralNet\n",
    "\n",
    "neuralNet = KerasClassifier(build_fn=cc_default_classifier)\n",
    "\n",
    "batch_sizes = [24, 32]\n",
    "epochs = [30, 50]\n",
    "params = {\n",
    "    'batch_size': batch_sizes,\n",
    "    'epochs': epochs,\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(neuralNet, params, verbose=2, cv=3)\n",
    "clf.fit(np.array(x_train), np.array(y_train))\n",
    "\n",
    "# Display the best score and best parameters\n",
    "print(\"Best mean test score and best parameters:\")\n",
    "print(clf.best_score_, clf.best_params_)\n",
    "print()\n",
    "\n",
    "# Loop through and display each pair of mean test score and parameter\n",
    "print(\"List of Mean test scores and respective parameters:\")\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "parameters = clf.cv_results_['params']\n",
    "for mean, parameter in zip(means, parameters):\n",
    "    print(mean, parameter)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tuning optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.constraints import unit_norm\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def cc_default_classifier(optimizer):\n",
    "    input_dim = x_train.shape[1]\n",
    "\n",
    "    # Weight constraints provide an approach to reduce the overfitting of a deep learning neural network model on the training data and improve the performance of the model on new data\n",
    "    neuralNet = Sequential()\n",
    "    neuralNet.add(Dense(64, input_shape=(input_dim,), activation='relu', kernel_constraint=unit_norm()))\n",
    "    neuralNet.add(Dropout(0.5))\n",
    "    neuralNet.add(Dense(32, activation='relu', kernel_constraint=unit_norm()))\n",
    "    neuralNet.add(Dropout(0.5))\n",
    "    neuralNet.add(Dense(32, activation='relu', kernel_constraint=unit_norm()))\n",
    "    neuralNet.add(Dropout(0.5))\n",
    "    neuralNet.add(Dense(16, activation='relu', kernel_constraint=unit_norm()))\n",
    "    neuralNet.add(Dropout(0.5))\n",
    "    neuralNet.add(Dense(1,  activation='sigmoid'))\n",
    "\n",
    "    neuralNet.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return neuralNet\n",
    "\n",
    "neuralNet = KerasClassifier(build_fn=cc_default_classifier, epochs=50, batch_size=24)\n",
    "\n",
    "params = {'optimizer':['SGD', 'Adagrad', 'Adam']}\n",
    "\n",
    "clf = GridSearchCV(neuralNet, params, verbose=2, cv=3)\n",
    "clf.fit(np.array(x_train), np.array(y_train))\n",
    "\n",
    "# Display the best score and best parameters\n",
    "print(\"Best mean test score and best parameters:\")\n",
    "print(clf.best_score_, clf.best_params_)\n",
    "print()\n",
    "\n",
    "# Loop through and display each pair of mean test score and parameter\n",
    "print(\"List of Mean test scores and respective parameters:\")\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "parameters = clf.cv_results_['params']\n",
    "for mean, parameter in zip(means, parameters):\n",
    "    print(mean, parameter)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.constraints import unit_norm\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "# Weight constraints provide an approach to reduce the overfitting of a deep learning neural network model on the training data and improve the performance of the model on new data\n",
    "neuralNet = Sequential()\n",
    "neuralNet.add(Dense(64, input_shape=(input_dim,), activation='relu', kernel_constraint=unit_norm()))\n",
    "neuralNet.add(Dropout(0.5))\n",
    "neuralNet.add(Dense(32, activation='relu', kernel_constraint=unit_norm()))\n",
    "neuralNet.add(Dropout(0.5))\n",
    "neuralNet.add(Dense(32, activation='relu', kernel_constraint=unit_norm()))\n",
    "neuralNet.add(Dropout(0.5))\n",
    "neuralNet.add(Dense(16, activation='relu', kernel_constraint=unit_norm()))\n",
    "neuralNet.add(Dropout(0.5))\n",
    "neuralNet.add(Dense(1,  activation='sigmoid'))\n",
    "\n",
    "neuralNet.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = neuralNet.fit(np.array(x_train), np.array(y_train),\n",
    "              batch_size=24, epochs=50, verbose=1,\n",
    "              validation_split=0.2)\n",
    "\n",
    "## evaluate the model performance and log the metrics\n",
    "# predict probabilities for test set\n",
    "y_pred_probs = neuralNet.predict(x_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "y_pred_classes = neuralNet.predict_classes(x_test, verbose=0)\n",
    "\n",
    "# reduce to 1d array\n",
    "y_pred_probs = y_pred_probs[:, 0]\n",
    "y_pred_classes = y_pred_classes[:, 0]\n",
    "\n",
    "metrics.loc['accuracy', 'NeuralNet'] = accuracy_score(y_test, y_pred_classes)\n",
    "metrics.loc['precision', 'NeuralNet'] = precision_score(y_test, y_pred_classes)\n",
    "metrics.loc['recall', 'NeuralNet'] = recall_score(y_test, y_pred_classes)\n",
    "metrics.loc['f1-score', 'NeuralNet'] = f1_score(y_test, y_pred_classes)\n",
    "metrics.loc['AUC', 'NeuralNet'] = roc_auc_score(y_test, y_pred_probs)\n",
    "\n",
    "# construct the confusion matrix\n",
    "CM = confusion_matrix(y_test, y_pred_classes)\n",
    "c_matrix(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "metrics.plot(kind='barh', ax=ax)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot roc curves for all models\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, log_reg.predict_proba(x_test)[:,1])\n",
    "fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, dec_tree.predict_proba(x_test)[:,1])\n",
    "fpr_nn, tpr_nn, thresholds_nn = roc_curve(y_test, neuralNet.predict(x_test, verbose=0)[:, 0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.plot([0, 1], [0, 1], linestyle='--')\n",
    "ax.plot(fpr_lr, tpr_lr, label='LogisticReg')\n",
    "ax.plot(fpr_dt, tpr_dt, label='DecisionTree')\n",
    "ax.plot(fpr_nn, tpr_nn, label='NeuralNet')\n",
    "ax.set_title('ROC Curves')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision-recall curves for all models\n",
    "precision_lr, recall_lr, thresholds_lr = precision_recall_curve(y_test, log_reg.predict_proba(x_test)[:,1])\n",
    "precision_dt, recall_dt, thresholds_dt = precision_recall_curve(y_test, dec_tree.predict_proba(x_test)[:,1])\n",
    "precision_nn, recall_nn, thresholds_nn = precision_recall_curve(y_test, neuralNet.predict(x_test, verbose=0)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.plot(precision_lr, recall_lr, label='LogisticReg')\n",
    "ax.plot(precision_dt, recall_dt, label='DecisionTree')\n",
    "ax.plot(precision_nn, recall_nn, label='NeuralNet')\n",
    "ax.set_xlabel('Precision')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.set_title('Precision-Recall Curves')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
