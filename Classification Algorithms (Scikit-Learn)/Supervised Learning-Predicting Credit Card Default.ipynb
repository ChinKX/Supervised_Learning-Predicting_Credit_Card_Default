{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('default_of_credit_card_clients.xls', index_col=\"ID\", skiprows=[0])\n",
    "\n",
    "#print(df.head())\n",
    "#print(df.describe())\n",
    "#print(df.info())\n",
    "\n",
    "# One Hot-Coding for categorical features : binary features take values of 1 or 0\n",
    "# - Scikit-learn might assume these are numerical features\n",
    "# - can't use labels because Scikit-learn only accepts numbers\n",
    "\n",
    "# obtain the one hot encoding of columns 'SEX', 'EDUCATION', 'MARRIAGE'\n",
    "# The base values are: female, other_education, other_marital_status\n",
    "df['male'] = (df['SEX'] == 1).astype('int')\n",
    "df.drop('SEX', axis=1, inplace=True)\n",
    "\n",
    "df['grad_school'] = (df['EDUCATION'] == 1).astype('int')\n",
    "df['university'] = (df['EDUCATION'] == 2).astype('int')\n",
    "df['high_school'] = (df['EDUCATION'] == 3).astype('int')\n",
    "df.drop('EDUCATION', axis=1, inplace=True)\n",
    "\n",
    "df['married'] = (df['MARRIAGE'] == 1).astype('int')\n",
    "df['single'] = (df['MARRIAGE'] == 2).astype('int')\n",
    "df.drop('MARRIAGE', axis=1, inplace=True)\n",
    "\n",
    "# From the documentation, we can infer that PAY_n features represent not delayed if it is <= 0\n",
    "pay_n_features = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "for col in pay_n_features:\n",
    "    hist = df[col].hist(bins=10)\n",
    "    print(\"Plotting for column {}\".format(col))\n",
    "    plt.show()\n",
    "    \n",
    "# modify all values of PAY_n features which are < 0 to 0\n",
    "for pay_n in pay_n_features:\n",
    "    df.loc[df[pay_n] <= 0, pay_n] = 0\n",
    "\n",
    "df.rename(columns={'default payment next month': 'default'}, inplace=True)\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling to get more accurate representation and better learning performance\n",
    "'''\n",
    "Most machine learning algorithms take into account only the magnitude of the measurements, not the units of those measurements.\n",
    "The feature with a very high magnitude (number) may affect the prediction a lot more than an equally important feature.\n",
    "e.g. the AGE (within certain fixed range) and the PAY_AMTn (monetary) features have very different ranges of values\n",
    "\n",
    "RobustScaler:\n",
    "The Robust Scaler uses statistics that are robust to outliers.\n",
    "This usage of interquartiles means that they focus on the parts where the bulk of the data is.\n",
    "This makes them very suitable for working with outliers.\n",
    "Notice that after Robust scaling, the distributions are brought into the same scale and overlap, but the outliers remain outside of bulk of the new distributions.\n",
    "'''\n",
    "# plot the distribution of all data\n",
    "for col in df.columns:\n",
    "    hist = df[col].hist(bins=10)\n",
    "    print(\"Plotting for column {}\".format(col))\n",
    "    plt.show()\n",
    "\n",
    "x = df.drop('default', axis=1)\n",
    "robust_scaler = RobustScaler()\n",
    "x = robust_scaler.fit_transform(x)# rescale all the features to a same range\n",
    "y = df['default']\n",
    "# stratify parameter makes data split in a stratified fashion meaning the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=123, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_matrix(CM, labels=['pay', 'default']):\n",
    "    df = pd.DataFrame(data = CM, index=labels, columns=labels)\n",
    "    df.index.name = 'TRUE'\n",
    "    df.columns.name = 'PREDICTION'\n",
    "    df.loc['Total'] = df.sum()\n",
    "    df['Total'] = df.sum(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing dataframe to store the evaluation metrics\n",
    "metrics = pd.DataFrame(\n",
    "    index=['accuracy', 'precision', 'recall'],\n",
    "    columns=['NULL', 'LogisticReg', 'DecisionTree', 'NaiveBayes']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this application\n",
    "\n",
    "1. Accuracy: Overall how often the model predicts correctly defaulters and non-defaulters?\n",
    "2. Precision: When the model predicts defaults: how often is correct?\n",
    "3. Recall: The proportion of actual defaulters that the model will correctly predict?\n",
    "\n",
    "### Which metric to use?\n",
    "1. False positive: A person who will pay predicted as defaulter\n",
    "2. False negative: A person who will default predicted as payer\n",
    "\n",
    "#### False negatives are worse => look for better recall\n",
    "\n",
    "## The Null model: always predict the most common category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark or base for how good the model must be performed to beat the Null model\n",
    "# predict the most common category which is 'pay'\n",
    "y_predicted = np.repeat(y_train.value_counts().idxmax(), y_test.size)\n",
    "metrics.loc['accuracy', 'NULL'] = accuracy_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['precision', 'NULL'] = precision_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['recall', 'NULL'] = recall_score(y_pred=y_predicted, y_true=y_test)\n",
    "\n",
    "# construct the confusion matrix\n",
    "CM = confusion_matrix(y_pred=y_predicted, y_true=y_test)\n",
    "c_matrix(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create an instance of the model\n",
    "log_reg = LogisticRegression(n_jobs=-1, random_state=15)\n",
    "\n",
    "# train the model using the training data\n",
    "log_reg.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "y_predicted = log_reg.predict(x_test)\n",
    "metrics.loc['accuracy', 'LogisticReg'] = accuracy_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['precision', 'LogisticReg'] = precision_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['recall', 'LogisticReg'] = recall_score(y_pred=y_predicted, y_true=y_test)\n",
    "\n",
    "# construct the confusion matrix\n",
    "CM = confusion_matrix(y_pred=y_predicted, y_true=y_test)\n",
    "c_matrix(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model class\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# create an instance of the model\n",
    "'''\n",
    "min_samples_split => minimum number of samples required to split an internal node\n",
    "min_samples_leaf => minimum number of samples required to be at a leaf node\n",
    "'''\n",
    "dec_tree = DecisionTreeClassifier(min_samples_split=30, min_samples_leaf=10, random_state=10)\n",
    "\n",
    "# train the model using the training data\n",
    "dec_tree.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "y_predicted = dec_tree.predict(x_test)\n",
    "metrics.loc['accuracy', 'DecisionTree'] = accuracy_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['precision', 'DecisionTree'] = precision_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['recall', 'DecisionTree'] = recall_score(y_pred=y_predicted, y_true=y_test)\n",
    "\n",
    "# construct the confusion matrix\n",
    "CM = confusion_matrix(y_pred=y_predicted, y_true=y_test)\n",
    "c_matrix(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model class\n",
    "from sklearn.naive_bayes import GaussianNB# for features with continuous values\n",
    "\n",
    "# create an instance of the model\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "# train the model using the training data\n",
    "nb_classifier.fit(x_train, y_train)\n",
    "\n",
    "# evaluate the model performance\n",
    "y_predicted = nb_classifier.predict(x_test)\n",
    "metrics.loc['accuracy', 'NaiveBayes'] = accuracy_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['precision', 'NaiveBayes'] = precision_score(y_pred=y_predicted, y_true=y_test)\n",
    "metrics.loc['recall', 'NaiveBayes'] = recall_score(y_pred=y_predicted, y_true=y_test)\n",
    "\n",
    "# construct the confusion matrix\n",
    "CM = confusion_matrix(y_pred=y_predicted, y_true=y_test)\n",
    "c_matrix(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "metrics.plot(kind='barh', ax=ax)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust precision and recall by modifying the classification thresholds\n",
    "# predict_proba gives you the probabilities for the target (0 and 1 in your case) in array form\n",
    "precision_nb, recall_nb, thresholds_nb = precision_recall_curve(y_true=y_test, probas_pred=nb_classifier.predict_proba(x_test)[:,1])\n",
    "\n",
    "precision_lr, recall_lr, thresholds_lr = precision_recall_curve(y_true=y_test, probas_pred=log_reg.predict_proba(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.plot(precision_nb, recall_nb, label='NaiveBayes')\n",
    "ax.plot(precision_lr, recall_lr, label='LogisticReg')\n",
    "ax.set_xlabel('Precision')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.set_title('Precision-Recall Curve')\n",
    "ax.hlines(y=0.5, xmin=0, xmax=1, color='red')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "# Logistic regression is better than Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for modified Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "print(thresholds_lr)\n",
    "print(precision_lr)\n",
    "ax.plot(thresholds_lr, precision_lr[1:], label='Precision')\n",
    "ax.plot(thresholds_lr, recall_lr[1:], label='Recall')\n",
    "ax.set_xlabel('Classfication Threshold')\n",
    "ax.set_ylabel('Precision, Recall')\n",
    "ax.set_title('Logistic Regression Classifier: Precision-Recall')\n",
    "ax.hlines(y=0.6, xmin=0, xmax=1, color='red')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier with threshold of 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = log_reg.predict_proba(x_test)[:,1]\n",
    "y_predicted = (y_pred_proba >= 0.2).astype('int')\n",
    "# adjust the original classification threshold from 0.5 to 0.2\n",
    "\n",
    "# confusion matrix\n",
    "CM = confusion_matrix(y_pred=y_predicted, y_true=y_test)\n",
    "print(\"Recall: \", 100*recall_score(y_pred=y_predicted, y_true=y_test))\n",
    "print(\"Precision: \", 100*precision_score(y_pred=y_predicted, y_true=y_test))\n",
    "c_matrix(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Predictive Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_default(new_data):\n",
    "    '''\n",
    "    #print(new_data)\n",
    "    #print(new_data.shape)\n",
    "    # The criterion to satisfy for providing the new shape is that 'The new shape should be compatible with the original shape'\n",
    "    # https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape\n",
    "    '''\n",
    "    data = new_data.values.reshape(1, -1)\n",
    "    data = robust_scaler.transform(data)\n",
    "    prob = log_reg.predict_proba(data)[0][1]\n",
    "    if prob >= 0.2:\n",
    "        return \"Will default\"\n",
    "    else:\n",
    "        return \"Will pay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay = df[df['default']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "new_customer = OrderedDict([\n",
    "    ('LIMIT_BAL', 4000), ('AGE', 50), ('BILL_AMT1', 500),\n",
    "    ('BILL_AMT2', 35509), ('BILL_AMT3', 689), ('BILL_AMT4', 0),\n",
    "    ('BILL_AMT5', 0), ('BILL_AMT6', 0), ('PAY_AMT1', 0),\n",
    "    ('PAY_AMT2', 35509), ('PAY_AMT3', 0), ('PAY_AMT4', 0),\n",
    "    ('PAY_AMT5', 0), ('PAY_AMT6', 0), ('male', 1), ('grad_school', 0),\n",
    "    ('university', 1), ('high_school', 0), ('married', 1), ('single', 0), ('pay_0', -1),\n",
    "    ('pay_2', -1), ('pay_3', -1), ('pay_4', 0), ('pay_5', -1), ('pay_6', 0),\n",
    "])\n",
    "\n",
    "new_customer = pd.Series(new_customer)\n",
    "predict_default(new_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for x in negative.index[0:100]:\n",
    "    print(predict_default(negative.loc[x].drop('default')))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
